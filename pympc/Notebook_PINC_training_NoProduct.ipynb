{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Physics-Informed Neural Network for control of dynamic system.\n",
    "\n",
    "Use case: _Fed-Batch bioreactor with no product formation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "from typing import Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import qmc\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#################################################################   \n",
    "####################Physics-Informed Neural Network #############\n",
    "NUM_EPOCHS = 100000\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_COLLOCATION = 200000\n",
    "PATIENCE = 1000\n",
    "THRESHOLD = 1e-3\n",
    "EARLY_STOPPING_EPOCH = 50000\n",
    "NUM_SAMPLES = 5000\n",
    "\n",
    "T_START = 0.0\n",
    "T_END = 5.0\n",
    "dt = 0.1\n",
    "\n",
    "X_MIN = 3.0\n",
    "X_MAX = 30.0\n",
    "S_MIN = 0.1\n",
    "S_MAX = 3.0\n",
    "V_MIN = 1.0\n",
    "V_MAX = 3.0\n",
    "F_MIN = 0.0\n",
    "F_MAX = 0.1\n",
    "\n",
    "# --- Model Parameters ---\n",
    "MU_MAX = 0.86980    # 1/h\n",
    "K_S = 0.000123762    # g/l\n",
    "Y_XS = 0.435749      # g/g\n",
    "S_F = 286           # g/l\n",
    "\n",
    "# Initial Conditions\n",
    "X_0, S_0, V_0 = 5, 0.013, 1.7  # Biomass, Substrate, Volume\n",
    "\n",
    "# ODE solver parameters\n",
    "ODE_SOLVER = 'LSODA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_tensor(array):\n",
    "    return torch.tensor(array, requires_grad=True, dtype=torch.float32).to(DEVICE).reshape(-1, 1)\n",
    "\n",
    "def grad(outputs, inputs):\n",
    "    return torch.autograd.grad(outputs.sum(), inputs, create_graph=True)[0]\n",
    "\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(input_dim, 64)\n",
    "        self.hidden1 = nn.Linear(64, 256)\n",
    "        self.hidden2 = nn.Linear(256, 64)\n",
    "        self.output = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.input(x))\n",
    "        x = torch.tanh(self.hidden1(x))\n",
    "        x = torch.tanh(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(num_samples: int = NUM_SAMPLES, sampling_method: str = 'lhs') -> tuple:\n",
    "    \"\"\"Generate dataset of random multiple initial conditions and control actions\"\"\"\n",
    "    df = pd.DataFrame(columns=['t', 'X', 'S', 'V', 'F'])\n",
    "    if sampling_method == 'uniform':\n",
    "        df['X'] = np.random.uniform(X_MIN, X_MAX, num_samples)\n",
    "        df['S'] = np.random.uniform(S_MIN, S_MAX, num_samples)\n",
    "        df['V'] = np.random.uniform(V_MIN, V_MAX, num_samples)\n",
    "        df['F'] = np.random.uniform(F_MIN, F_MAX, num_samples)\n",
    "        df['t'] = 0.0 # initial time (always 0)\n",
    "    elif sampling_method == 'lhs':\n",
    "        sampler = qmc.LatinHypercube(d=4)\n",
    "        lhs_samples = sampler.random(n=num_samples)\n",
    "        scaled_samples = qmc.scale(lhs_samples, [X_MIN, S_MIN, V_MIN, F_MIN], [X_MAX, S_MAX, V_MAX, F_MAX])\n",
    "        df = pd.DataFrame(scaled_samples, columns=['X', 'S', 'V', 'F'])\n",
    "        df['t'] = 0.0\n",
    "        \n",
    "    t_train = numpy_to_tensor(df['t'].values)\n",
    "    X_train = numpy_to_tensor(df['X'].values)\n",
    "    S_train = numpy_to_tensor(df['S'].values)\n",
    "    V_train = numpy_to_tensor(df['V'].values)\n",
    "    F_train = numpy_to_tensor(df['F'].values)\n",
    "    \n",
    "    in_train = torch.cat([t_train, X_train, S_train, V_train, F_train], dim=1)\n",
    "    out_train = torch.cat([X_train, S_train, V_train], dim=1)\n",
    "    \n",
    "    return in_train, out_train\n",
    "\n",
    "def loss_fn(net: nn.Module, sampling_method: str = 'lhs') -> torch.Tensor:\n",
    "    if sampling_method == 'uniform':\n",
    "        t_col = numpy_to_tensor(np.random.uniform(T_START, dt, NUM_COLLOCATION))\n",
    "        X0_col = numpy_to_tensor(np.random.uniform(X_MIN, X_MAX, NUM_COLLOCATION))\n",
    "        S0_col = numpy_to_tensor(np.random.uniform(S_MIN, S_MAX, NUM_COLLOCATION))\n",
    "        V0_col = numpy_to_tensor(np.random.uniform(V_MIN, V_MAX, NUM_COLLOCATION))\n",
    "        F_col  = numpy_to_tensor(np.random.uniform(F_MIN, F_MAX, NUM_COLLOCATION))\n",
    "    elif sampling_method == 'lhs':\n",
    "        sampler = qmc.LatinHypercube(d=5)\n",
    "        lhs_samples = sampler.random(n=NUM_COLLOCATION)\n",
    "        scaled_samples = qmc.scale(lhs_samples, [T_START, X_MIN, S_MIN, V_MIN, F_MIN], [dt, X_MAX, S_MAX, V_MAX, F_MAX])\n",
    "        t_col = numpy_to_tensor(scaled_samples[:, 0])\n",
    "        X0_col = numpy_to_tensor(scaled_samples[:, 1])\n",
    "        S0_col = numpy_to_tensor(scaled_samples[:, 2])\n",
    "        V0_col = numpy_to_tensor(scaled_samples[:, 3])\n",
    "        F_col  = numpy_to_tensor(scaled_samples[:, 4])\n",
    "    \n",
    "    u_col = torch.cat([t_col, X0_col, S0_col, V0_col, F_col], dim=1)\n",
    "    preds = net.forward(u_col)\n",
    "\n",
    "    X_pred = preds[:, 0].view(-1, 1)\n",
    "    S_pred = preds[:, 1].view(-1, 1)\n",
    "    V_pred = preds[:, 2].view(-1, 1)\n",
    "\n",
    "    dXdt_pred = grad(X_pred, t_col)\n",
    "    dSdt_pred = grad(S_pred, t_col) \n",
    "    dVdt_pred = grad(V_pred, t_col) # not used\n",
    "\n",
    "    mu = MU_MAX * S_pred / (K_S + S_pred)\n",
    "\n",
    "    # residuals\n",
    "    rhs_X = mu * X_pred - (F_col / (V_pred + 1e-6)) * X_pred\n",
    "    rhs_S = - (mu * X_pred) / Y_XS + (F_col / (V_pred + 1e-6)) * (S_F - S_pred)\n",
    "    rhs_V = F_col\n",
    "\n",
    "    error_dXdt = dXdt_pred - rhs_X\n",
    "    error_dSdt = dSdt_pred - rhs_S\n",
    "    error_dVdt = dVdt_pred - rhs_V\n",
    "\n",
    "    # average residual squared loss\n",
    "    loss_ode = torch.mean(error_dXdt**2) + torch.mean(error_dSdt**2) + torch.mean(error_dVdt**2)\n",
    "\n",
    "    return loss_ode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_train, out_train = generate_dataset()\n",
    "\n",
    "print(f'Input shape: {in_train.shape}')\n",
    "print(f'Output shape: {out_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"pinc_model_br.pth\"\n",
    "model_exists = os.path.exists(f'./models/{model_name}')\n",
    "\n",
    "if model_exists:\n",
    "    # Load the model\n",
    "    net = PINN(input_dim=in_train.shape[1], output_dim=out_train.shape[1]).to(DEVICE)\n",
    "    net.load_state_dict(torch.load(f'./models/{model_name}', weights_only=True))\n",
    "    net.eval()\n",
    "else:\n",
    "    # Main\n",
    "    net = PINN(input_dim=in_train.shape[1], output_dim=out_train.shape[1]).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5000, gamma=0.75)\n",
    "\n",
    "    # Loss weights\n",
    "    w_data, w_ode, w_ic = 1.0, 1.5, 1.0\n",
    "\n",
    "    # Initialize early stopping variables\n",
    "    best_loss = float(\"inf\")\n",
    "    best_model_weights = None\n",
    "    patience = PATIENCE\n",
    "    threshold = THRESHOLD\n",
    "    pretrain_epochs = 10\n",
    "\n",
    "    for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "        optimizer.zero_grad()\n",
    "        preds = net.forward(in_train)\n",
    "        X_pred = preds[:, 0].view(-1, 1)\n",
    "        S_pred = preds[:, 1].view(-1, 1)\n",
    "        V_pred = preds[:, 2].view(-1, 1)\n",
    "        loss_data = torch.mean((X_pred - out_train[:, 0].view(-1, 1))**2) + torch.mean((S_pred - out_train[:, 1].view(-1, 1))**2) + torch.mean((V_pred - out_train[:, 2].view(-1, 1))**2)\n",
    "        \n",
    "        if epoch < pretrain_epochs:\n",
    "            loss = loss_data * w_data\n",
    "        else:\n",
    "            loss_ode = loss_fn(net)\n",
    "            loss = w_data * loss_data + w_ode * loss_ode\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if epoch % 1000 == 0:\n",
    "            if epoch < pretrain_epochs:\n",
    "                print(f\"Epoch {epoch}: Loss = {loss_data.item():.4f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch}: Loss = {loss.item():.4f}, Data Loss = {loss_data.item():.4f}, ODE Loss = {loss_ode.item():.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch >= EARLY_STOPPING_EPOCH:\n",
    "            if loss.item() < best_loss - threshold:\n",
    "                best_loss = loss.item()\n",
    "                best_model_weights = copy.deepcopy(net.state_dict())\n",
    "                patience = PATIENCE\n",
    "            else:\n",
    "                patience -= 1\n",
    "                if patience <= 0:\n",
    "                    print(f\"Early stopping at epoch {epoch}. Best loss: {best_loss:.4f} at epoch {epoch - PATIENCE}.\")\n",
    "                    break\n",
    "\n",
    "    # Load best model weights\n",
    "    if best_model_weights is not None:\n",
    "        net.load_state_dict(best_model_weights)\n",
    "        net.eval()\n",
    "        print(\"Loaded best model weights.\")\n",
    "    else:\n",
    "        print(\"No model weights to load.\")\n",
    "        net.eval()\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(net.state_dict(), f'./models/{model_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_lbfgs(net: nn.Module):\n",
    "    # Create a new optimizer for L-BFGS\n",
    "    optimizer = torch.optim.LBFGS(net.parameters(), lr=LEARNING_RATE, max_iter=NUM_EPOCHS, history_size=10, line_search_fn='strong_wolfe')\n",
    "\n",
    "    # Fine tuning using L-BFGS\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        preds = net.forward(in_train)\n",
    "        X_pred = preds[:, 0].view(-1, 1)\n",
    "        S_pred = preds[:, 1].view(-1, 1)\n",
    "        loss_data = (torch.mean((X_pred - out_train[:, 0].view(-1, 1))**2) + \n",
    "                    torch.mean((S_pred - out_train[:, 1].view(-1, 1))**2)) / 2\n",
    "        loss_ode = loss_fn(net)\n",
    "        loss = loss_data + loss_ode\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    # Set the model to training mode\n",
    "    net.train()\n",
    "    # Run the L-BFGS optimization\n",
    "    for epoch in tqdm(range(10)):\n",
    "        optimizer.step(closure)\n",
    "        if epoch % 100 == 0:\n",
    "            loss = closure()\n",
    "            print(f\"Epoch {epoch}: Loss = {loss.item():.4e}\")\n",
    "        \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "net = PINN(input_dim=in_train.shape[1], output_dim=out_train.shape[1]).to(DEVICE)\n",
    "net.load_state_dict(torch.load(f'./models/{model_name}', weights_only=True))\n",
    "net.eval()\n",
    "\n",
    "def flow_rate(t: float) -> float:\n",
    "    \"\"\"Flow rate as a function of time\"\"\"\n",
    "    if t < 0.5:\n",
    "        return 0.01\n",
    "    elif t < 5.0:\n",
    "        return 0.1\n",
    "    else:\n",
    "        return 0.00\n",
    "\n",
    "# System dynamics\n",
    "def system_dynamics(t: float, y: float, F: Union[callable, float]) -> np.array:\n",
    "    X, S, V = y\n",
    "    Fs = F(t) if callable(F) else F\n",
    "    dX_dt = (MU_MAX * S / (K_S + S)) * X - (Fs / V) * X\n",
    "    dS_dt = -(1 / Y_XS) * (MU_MAX * S / (K_S + S)) * X + (Fs / V) * (S_F - S)\n",
    "    dV_dt = Fs\n",
    "    return np.array([dX_dt, dS_dt, dV_dt])\n",
    "\n",
    "def solve_ode(ode: callable, t_span: list, y0: list, method: str, t_eval: list, args: tuple) -> solve_ivp:\n",
    "    sol = solve_ivp(ode, t_span=t_span, y0=y0, method=method, args=args, t_eval=t_eval, rtol=1e-6, atol=1e-12)\n",
    "    # sol.y = np.maximum(sol.y, 0)\n",
    "    return sol\n",
    "\n",
    "sol = solve_ode(system_dynamics, t_span=[T_START, T_END], y0=[X_0, S_0, V_0], method=ODE_SOLVER, \\\n",
    "                t_eval=np.arange(T_START, T_END, dt), args=(flow_rate,))\n",
    "df = pd.DataFrame(sol.y.T, columns=['X', 'S', 'V'])\n",
    "df['Time'] = sol.t\n",
    "\n",
    "T_s = 0.1\n",
    "t_test = numpy_to_tensor(np.array([df['Time'].values]))\n",
    "X_test = numpy_to_tensor(np.array([df['X'].values]))\n",
    "S_test = numpy_to_tensor(np.array([df['S'].values]))\n",
    "V_test = numpy_to_tensor(np.array([df['V'].values]))\n",
    "F_test = numpy_to_tensor(np.array([flow_rate(t) for t in df['Time'].values]))\n",
    "u_test = torch.cat([t_test, X_test, S_test, V_test, F_test], dim=1)\n",
    "x_test = torch.cat([X_test, S_test, V_test], dim=1)\n",
    "\n",
    "X_0 = X_test[0]\n",
    "S_0 = S_test[0]\n",
    "V_0 = V_test[0]\n",
    "F_0 = F_test[0]\n",
    "\n",
    "X_preds = []\n",
    "S_preds = []\n",
    "V_preds = []\n",
    "\n",
    "net.eval()\n",
    "for i in range(len(u_test)):\n",
    "\n",
    "    if i == 0:\n",
    "        X_preds.append(X_0.item())\n",
    "        S_preds.append(S_0.item())\n",
    "        V_preds.append(V_0.item())\n",
    "        continue\n",
    "\n",
    "    x_k = net.forward(torch.tensor([T_s, X_0, S_0, V_0, F_0], dtype=torch.float32).to(DEVICE))\n",
    "    X_0 = X_test[i]\n",
    "    S_0 = S_test[i]\n",
    "    V_0 = V_test[i]\n",
    "    F_0 = F_test[i]\n",
    "    X_preds.append(x_k[0].item())\n",
    "    S_preds.append(x_k[1].item())\n",
    "    V_preds.append(x_k[2].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(df['Time'], df['X'], label='X (g/L)', color='blue', s=10)\n",
    "plt.scatter(df['Time'], df['S'], label='S (g/L)', color='orange', s=10)\n",
    "\n",
    "plt.plot(t_test.detach().cpu().numpy(), X_preds, marker='x', label=\"_X_pred\")\n",
    "plt.plot(t_test.detach().cpu().numpy(), S_preds, marker='x', label=\"_S_pred\")\n",
    "plt.ylabel(\"Concentration (g/L)\")\n",
    "plt.legend(loc=\"best\") \n",
    "\n",
    "plt.xticks(np.arange(0, 6, 0.5))\n",
    "plt.xlabel(\"Time (h)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
